{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First look on all the data at once\n",
    "\n",
    "### Doing basic plots for all 10 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.dates as mdates\n",
    "from arch.unitroot import PhillipsPerron\n",
    "from arch import arch_model\n",
    "\n",
    "# from arch.univariate import EGARCH\n",
    "# from arch.univariate import ConstantMean\n",
    "# from arch.univariate import Normal\n",
    "\n",
    "\n",
    "# pair_list = [\n",
    "#             (\"SHIB-WETH\", \"https://www.dextools.io/app/en/ether/pair-explorer/0x811beed0119b4afce20d2583eb608c6f7af1954f\", 50),\n",
    "#             (\"HEX-WETH\", \"https://www.dextools.io/app/en/ether/pair-explorer/0x55d5c232d921b9eaa6b37b5845e439acd04b4dba\", 100),\n",
    "#             (\"AGIX-WETH\", \"https://www.dextools.io/app/en/ether/pair-explorer/0xe45b4a84e0ad24b8617a489d743c52b84b7acebe\", 40),\n",
    "#             (\"OPTIMUS-WETH\", \"https://www.dextools.io/app/en/ether/pair-explorer/0x8de7a9540e0edb617d78ca5a7c6cc18295fd8bb9\", 70),\n",
    "#             (\"SHIK-WETH\", \"https://www.dextools.io/app/en/ether/pair-explorer/0x0b9f5cef1ee41f8cccaa8c3b4c922ab406c980cc\", 60),\n",
    "#             (\"INJ-WBNB\", \"https://www.dextools.io/app/en/bnb/pair-explorer/0x1bdcebca3b93af70b58c41272aea2231754b23ca\", 60),\n",
    "#             (\"VOLT-WBNB\", \"https://www.dextools.io/app/en/bnb/pair-explorer/0x487bfe79c55ac32785c66774b597699e092d0cd9\", 200),\n",
    "#             (\"MBOX-WBNB\", \"https://www.dextools.io/app/en/bnb/pair-explorer/0x8fa59693458289914db0097f5f366d771b7a7c3f\", 90),\n",
    "#             (\"FLOKI-WBNB\", \"https://www.dextools.io/app/en/bnb/pair-explorer/0x231d9e7181e8479a8b40930961e93e7ed798542c\", 180),\n",
    "#             (\"BabyDoge-WBNB\", \"https://www.dextools.io/app/en/bnb/pair-explorer/0xc736ca3d9b1e90af4230bd8f9626528b3d4e0ee0\", 180)\n",
    "#             ]\n",
    "\n",
    "pair_list = [\n",
    "            (\"AGIX-WETH\", \"https://www.dextools.io/app/en/ether/pair-explorer/0xe45b4a84e0ad24b8617a489d743c52b84b7acebe\", 40),\n",
    "            (\"HEX-WETH\", \"https://www.dextools.io/app/en/ether/pair-explorer/0x55d5c232d921b9eaa6b37b5845e439acd04b4dba\", 100),\n",
    "            (\"OPTIMUS-WETH\", \"https://www.dextools.io/app/en/ether/pair-explorer/0x8de7a9540e0edb617d78ca5a7c6cc18295fd8bb9\", 70),\n",
    "            (\"SHIB-WETH\", \"https://www.dextools.io/app/en/ether/pair-explorer/0x811beed0119b4afce20d2583eb608c6f7af1954f\", 50),\n",
    "            (\"SHIK-WETH\", \"https://www.dextools.io/app/en/ether/pair-explorer/0x0b9f5cef1ee41f8cccaa8c3b4c922ab406c980cc\", 60),\n",
    "            (\"BabyDoge-WBNB\", \"https://www.dextools.io/app/en/bnb/pair-explorer/0xc736ca3d9b1e90af4230bd8f9626528b3d4e0ee0\", 180),\n",
    "            (\"FLOKI-WBNB\", \"https://www.dextools.io/app/en/bnb/pair-explorer/0x231d9e7181e8479a8b40930961e93e7ed798542c\", 180),\n",
    "            (\"INJ-WBNB\", \"https://www.dextools.io/app/en/bnb/pair-explorer/0x1bdcebca3b93af70b58c41272aea2231754b23ca\", 60),\n",
    "            (\"MBOX-WBNB\", \"https://www.dextools.io/app/en/bnb/pair-explorer/0x8fa59693458289914db0097f5f366d771b7a7c3f\", 90),\n",
    "            (\"VOLT-WBNB\", \"https://www.dextools.io/app/en/bnb/pair-explorer/0x487bfe79c55ac32785c66774b597699e092d0cd9\", 200)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv_to_df(file_path, verbose=False, name=\"\"):\n",
    "    # Load CSV file into a DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # If verbose mode is enabled, print the dataset name, number of rows, and some datetime information\n",
    "    if verbose:\n",
    "        #print(name)\n",
    "        print(\"Number of rows:\", len(df))\n",
    "        print(\"Last datetime:\", df.datetime.iloc[-2], )  # Assuming there is a \"datetime\" column in the DataFrame\n",
    "        print(\"First datetime:\", df.datetime.iloc[0])\n",
    "        print(\"Time difference (in days):\", datetime.strptime(df.datetime.iloc[0], '%Y-%m-%d %H:%M:%S') - datetime.strptime(df.datetime.iloc[-5], '%Y-%m-%d %H:%M:%S'))\n",
    "        print()\n",
    "    \n",
    "    # Return the DataFrame\n",
    "    return df\n",
    "\n",
    "def set_index(df, info=False):\n",
    "    # Convert 'datetime' column to datetime type and set it as the index\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    if info:\n",
    "        # Print some information about the resulting DataFrame\n",
    "        df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting function\n",
    "#current version\n",
    "def plot_price(df, native=True, name=\"\", ax=None, plot_both=False):\n",
    "    # Choose the column to plot based on native argument\n",
    "    if native:\n",
    "        price_column = 'price_native'\n",
    "        column_name = 'Price (native token)'\n",
    "    else:\n",
    "        price_column = 'price_USD'\n",
    "        column_name = 'Price (USD)'\n",
    "    \n",
    "    # Plot the chosen column as a timeseries\n",
    "    if ax is None and not plot_both:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    if not plot_both:\n",
    "        df[price_column].plot(ax=ax)\n",
    "    \n",
    "        # Set the title, axis labels\n",
    "        ax.set_title(f'{name} {column_name} Timeseries')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel(column_name)\n",
    "    \n",
    "    # Optionally plot both timeseries side by side\n",
    "    if plot_both:\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "        plot_price(df, native=True, name=name, ax=ax1, plot_both=False)\n",
    "        plot_price(df, native=False, name=name, ax=ax2, plot_both=False)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get returns\n",
    "def add_returns(df, price_col='price_native', ret_col='return', log_ret_col='log_return'):\n",
    "    \"\"\"\n",
    "    Adds returns and log returns columns to a Pandas DataFrame based on a price series.\n",
    "    \n",
    "    Parameters:\n",
    "    df (Pandas DataFrame): The DataFrame to which the columns will be added.\n",
    "    price_col (str): The name of the column containing the price series (default: 'price').\n",
    "    ret_col (str): The name of the column to store the simple returns (default: 'return').\n",
    "    log_ret_col (str): The name of the column to store the log returns (default: 'log_return').\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate simple returns\n",
    "    df[ret_col] = df[price_col].pct_change()\n",
    "    \n",
    "    # Calculate log returns\n",
    "    df[log_ret_col] = np.log(df[price_col] / df[price_col].shift(1))\n",
    "    \n",
    "    # Drop the first row (which will have NaN values due to the calculation of returns)\n",
    "    df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_returns(df, native=True, log=True, name = \"\", lower_percentile=0, higher_percentile=100, histogram = False, bins = 20, \n",
    "                 buy_only = False, sell_only = False, set_hist_limit = False, lower_limit_abs = -1, higher_limit_abs = 1):\n",
    "    \"\"\"\n",
    "    Plots the returns or log returns columns of a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (Pandas DataFrame): The DataFrame containing the returns columns to be plotted.\n",
    "    native (bool): Whether to plot the native currency returns (default: True).\n",
    "    log (bool): Whether to plot the log returns (default: True).\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Choose the columns to plot based on native and log arguments\n",
    "    if native and log:\n",
    "        ret_col = 'log_return_native'\n",
    "        title = 'Log Returns (native token)'\n",
    "    elif native and not log:\n",
    "        ret_col = 'return_native'\n",
    "        title = 'Simple Returns (native token)'\n",
    "    elif not native and log:\n",
    "        ret_col = 'log_return_USD'\n",
    "        title = 'Log Returns (USD)'\n",
    "    else:\n",
    "        ret_col = 'return_USD'\n",
    "        title = 'Simple Returns (USD)'\n",
    "\n",
    "    # Subset DataFrame based on \"buy_order\" column if specified\n",
    "    if buy_only:\n",
    "        df = df[df[\"buy_order\"] == True]\n",
    "        title = name + \"BUYS \" + title\n",
    "    elif sell_only:\n",
    "        df = df[df[\"buy_order\"] == False]\n",
    "        title = name + \"SELLS \" + title\n",
    "    else:\n",
    "        title = name + \" \" + title\n",
    "\n",
    "    # Create the plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    plot_data = df[ret_col]\n",
    "    # lower_limit = np.percentile(plot_data, lower_percentile)*1.1\n",
    "    # higher_limit = np.percentile(plot_data, higher_percentile)*1.1\n",
    "    if not histogram:\n",
    "        lower_limit = np.percentile(plot_data, lower_percentile)*1.1\n",
    "        higher_limit = np.percentile(plot_data, higher_percentile)*1.1\n",
    "        plot_data.plot(ax=ax)\n",
    "        ax.set_ylim(lower_limit, higher_limit)\n",
    "        # Set the title and axis labels\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Return')\n",
    "    else:\n",
    "        plot_data.hist(ax=ax, bins=bins, edgecolor='black')\n",
    "        if set_hist_limit:\n",
    "            ax.set_xlim(lower_limit_abs, higher_limit_abs)\n",
    "        # Set the title and axis labels\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Return')\n",
    "        ax.set_ylabel('Count')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(df, native=True, log=True, name = \"\", quantiles = [0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99]):\n",
    "    \"\"\"\n",
    "    Prints the minimum, maximum, and quantiles of a column in a Pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (Pandas DataFrame): The DataFrame containing the column to be analyzed.\n",
    "    col (str): The name of the column to be analyzed.\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if native and log:\n",
    "        ret_col = 'log_return_native'\n",
    "        title = 'Log Returns (native currency)'\n",
    "    elif native and not log:\n",
    "        ret_col = 'return_native'\n",
    "        title = 'Simple Returns (native currency)'\n",
    "    elif not native and log:\n",
    "        ret_col = 'log_return_USD'\n",
    "        title = 'Log Returns (USD)'\n",
    "    else:\n",
    "        ret_col = 'return_USD'\n",
    "        title = 'Simple Returns (USD)'\n",
    "    print(title)\n",
    "    print(df[ret_col].min())\n",
    "    q = df[ret_col].quantile(quantiles)\n",
    "    print(q)\n",
    "    print(df[ret_col].max(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploting ACF and PACF\n",
    "def plot_acf_pacf(data, lags=35, figsize=(9, 8)):\n",
    "    \"\"\"\n",
    "    Plots Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) for a given time series data.\n",
    "    \n",
    "    Args:\n",
    "    data (array-like): Time series data.\n",
    "    lags (int): Number of lags to include in ACF and PACF plots. Default is 35.\n",
    "    figsize (tuple): Figure size of the plots. Default is (9, 8).\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    f, ax = plt.subplots(nrows=2, ncols=1, figsize=figsize)\n",
    "    plot_acf(data, lags=lags, ax=ax[0])\n",
    "    plot_pacf(data, lags=lags, ax=ax[1], method='ols')\n",
    "    #https://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html #link for the function\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the spacing between observations\n",
    "def plot_observation_spacing(df, name = \"\"):\n",
    "    \"\"\"\n",
    "    Plots a histogram of spacing between observations in seconds.\n",
    "    \n",
    "    Args:\n",
    "    - df (pd.DataFrame): Input DataFrame with DateTimeIndex.\n",
    "    \n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Calculate spacing between observations\n",
    "    spacing = np.diff(df.index).astype('timedelta64[s]').astype(int) *-1\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.hist(spacing, bins=100, edgecolor='black')\n",
    "    #plt.xlim(0, 20)\n",
    "    plt.xlabel('Spacing between observations (seconds)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(name + ' Histogram of Spacing between Observations')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the overview table\n",
    "def get_overview_table(df, name, column = \"return_native\", table = None):\n",
    "    if table is None:\n",
    "        # Create an empty DataFrame with the specified columns\n",
    "        table = pd.DataFrame(columns=[\"name\", \"observations\", \"mean\", \"median\", \"StD\", \"CV\", \"maximum\", \"minimum\", \"skewness\", \"kurtosis\", \"jarque_bera\", \"ADF\"])\n",
    "\n",
    "    # Compute statistics for the given column\n",
    "    observations = df[column].count()\n",
    "    mean = df[column].mean()\n",
    "    std_dev = df[column].std()\n",
    "    median = df[column].median()\n",
    "    CV = std_dev/mean\n",
    "    maximum = df[column].max()\n",
    "    minimum = df[column].min()\n",
    "    kurtosis = stats.kurtosis(df[column], fisher=False)\n",
    "    skewness = stats.skew(df[column])\n",
    "    jarque_bera = stats.jarque_bera(df[column])[0]\n",
    "    adf = adfuller(df[column])[0]\n",
    "\n",
    "    # Create a DataFrame for the computed statistics\n",
    "    stats_df = pd.DataFrame({\"name\": [name], \"observations\": [observations], \"mean\": [mean], \"CV\": [CV], \"StD\":[std_dev], \"median\": [median], \"maximum\": [maximum],\n",
    "                             \"minimum\": [minimum], \"kurtosis\": [kurtosis], \"skewness\": [skewness], \"jarque_bera\": [jarque_bera], \"ADF\": [adf]})\n",
    "\n",
    "    # Concatenate the computed statistics DataFrame with the overview table\n",
    "    table = pd.concat([table, stats_df], ignore_index=True)\n",
    "\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGIX-WETH\n",
      "HEX-WETH\n",
      "OPTIMUS-WETH\n",
      "SHIB-WETH\n",
      "SHIK-WETH\n",
      "BabyDoge-WBNB\n",
      "FLOKI-WBNB\n",
      "INJ-WBNB\n",
      "MBOX-WBNB\n",
      "VOLT-WBNB\n",
      "            name observations          mean        median           StD  \\\n",
      "0      AGIX-WETH         3693  2.733585e-04  2.766000e-04  2.473117e-05   \n",
      "1       HEX-WETH        14544  5.227032e-05  5.008000e-05  7.692040e-06   \n",
      "2   OPTIMUS-WETH         8939  2.290444e-04  2.204000e-04  3.570155e-05   \n",
      "3      SHIB-WETH         4754  6.073584e-10  6.064500e-10  1.241776e-11   \n",
      "4      SHIK-WETH         4310  2.355794e-13  2.328000e-13  4.424797e-14   \n",
      "5  BabyDoge-WBNB        18441  7.815328e-13  7.686000e-13  7.389323e-14   \n",
      "6     FLOKI-WBNB        26364  1.114808e-08  1.111000e-08  4.373027e-10   \n",
      "7       INJ-WBNB         7520  1.371826e-02  1.335000e-02  1.436824e-03   \n",
      "8      MBOX-WBNB        12089  1.505814e-03  1.510000e-03  2.900167e-05   \n",
      "9      VOLT-WBNB        18006  4.655294e-10  4.495000e-10  3.303562e-11   \n",
      "\n",
      "         CV       maximum       minimum  skewness  kurtosis  jarque_bera  \\\n",
      "0  0.090472  4.122000e-04  2.214000e-04 -0.214957  2.340543    95.357736   \n",
      "1  0.147159  1.170000e-04  8.681000e-07  0.947621  3.688889  2464.305099   \n",
      "2  0.155872  3.336000e-04  1.507000e-04  0.618720  2.499987   663.448393   \n",
      "3  0.020446  7.418000e-10  5.735000e-10  0.590322  5.611417  1626.941493   \n",
      "4  0.187826  3.739000e-13  1.518000e-13  0.400999  2.593084   145.243850   \n",
      "5  0.094549  8.979000e-13  6.829000e-13  0.263261  1.495834  1951.473587   \n",
      "6  0.039227  1.225000e-08  1.010000e-08 -0.067486  1.983032  1156.106867   \n",
      "7  0.104738  1.887000e-02  1.164000e-02  0.870562  2.787656   964.002249   \n",
      "8  0.019260  1.568000e-03  1.433000e-03 -0.399063  2.414473   493.557544   \n",
      "9  0.070964  5.602000e-10  4.347000e-10  1.658784  4.439656  9812.413854   \n",
      "\n",
      "        ADF  \n",
      "0 -1.708306  \n",
      "1 -2.123219  \n",
      "2 -2.778502  \n",
      "3 -2.876444  \n",
      "4 -2.286404  \n",
      "5 -1.257904  \n",
      "6 -2.474002  \n",
      "7 -1.803423  \n",
      "8 -1.791087  \n",
      "9  1.333917  \n"
     ]
    }
   ],
   "source": [
    "#loop through all 10 datasets\n",
    "table = None\n",
    "for pair in pair_list:\n",
    "    #geting name of current pair and folder location\n",
    "    name = pair[0]\n",
    "    print(name)\n",
    "    file_path = f\"D:/Dokumenty/Vejška/Magisterské studium/DIPLOMKA/Code_and_Data/Data_scraping/DEX_data_scraper/complete_data/{name}/{name}_complete.csv\"\n",
    "\n",
    "    #loading data without werbose printout, seting index\n",
    "    df = load_csv_to_df(file_path, verbose=False, name = name)\n",
    "    set_index(df, info=False)\n",
    "\n",
    "    #printing info on column of a df\n",
    "    #print(df.info())\n",
    "\n",
    "    ### Basic block of plots ###\n",
    "    #Price plots\n",
    "    #plot_price(df, native=True, name=name, ax=None, plot_both=True)\n",
    "  \n",
    "    #adding retruns and plotting them\n",
    "    add_returns(df, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    add_returns(df, price_col='price_USD', ret_col='return_USD', log_ret_col='log_return_USD')\n",
    "    \n",
    "    # plot_returns(df, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=True, bins=100)\n",
    "    # plot_returns(df, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=False, bins=100)\n",
    "    # plot_returns(df, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=True, bins=100, \n",
    "    #              set_hist_limit=False, lower_limit_abs=-0.2, higher_limit_abs=0.2)\n",
    "    # print_stats(df, native=True, log=False, name=name)\n",
    "    \n",
    "    ### Histograms with separate Buy/Sell groups ###\n",
    "    #adding retruns and plotting them in histogram with buy/sell separated\n",
    "    # add_returns(df, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    # add_returns(df, price_col='price_USD', ret_col='return_USD', log_ret_col='log_return_USD')\n",
    "    # plot_returns(df, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=True, bins=100, buy_only = False, sell_only = False)\n",
    "    # plot_returns(df, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=True, bins=100, buy_only = True, sell_only = False)\n",
    "    # plot_returns(df, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=True, bins=100, buy_only = False, sell_only = True)\n",
    "\n",
    "\n",
    "    ### Test Statistics ###\n",
    "    ##Jarque-Bera test -- H0: Skewness and Kurtosis match Normal distribution - strongly rejected\n",
    "    # print(stats.jarque_bera(df.return_native))\n",
    "\n",
    "    ##ADF test (Augment Dickey-Fuller) -- H0: there is a unit root present (i.e. series is not stationary) - strongly rejected\n",
    "    # result = adfuller(df.return_native)\n",
    "    # print('ADF Statistic: %f' % result[0], 'p-value: %f' % result[1])\n",
    "\n",
    "    ##KPSS test for stationarity: H0 - The time series is trend stationary., H1 - not trend stationary. Failed to reject in all cases EXCEPT VOLT!!\n",
    "    # kpss_result = kpss(df.return_native)\n",
    "    # print(f\"KPSS test statistic {kpss_result[0]} with p-value {kpss_result[1]} on {kpss_result[2]} lags.\")\n",
    "\n",
    "    ##Durbin-Watson test of autocorrelation - H0 no autocorrelation, values around 2, H1 autocorrelation, close to 0 - positive, close to 4 negative\n",
    "    ##rule of thumb: test statistic values between the range of 1.5 and 2.5 are considered normal\n",
    "    # print(durbin_watson(df.return_native)) #expect not to reject H0\n",
    "    # print(durbin_watson(np.abs(df.return_native))) #expect to reject H0 for absolute values of returns - other form of clustering of Realized Variance\n",
    "    # print(\"\\n\")\n",
    "    \n",
    "    table = get_overview_table(df, name=name, column = \"price_native\", table = table)\n",
    "    \n",
    "    ### ACF and PACF ###\n",
    "    #plot_acf_pacf(df.log_return_USD, lags=35, figsize=(9, 8))\n",
    "\n",
    "    ### Get the histogram of time intervals ###\n",
    "    #plot_observation_spacing(df, name=name)\n",
    "\n",
    "\n",
    "\n",
    "    ###Ad Hoc plot of prices with separate buy/sell orders\n",
    "    # Filter the data based on the buy_order column\n",
    "    # data_buy = df[df['buy_order'] == True]#.iloc[300:600,:]\n",
    "    # data_sell = df[df['buy_order'] == False]#.iloc[300:600,:]\n",
    "\n",
    "    # # Create a plot of the returns_native column for both subsets of data\n",
    "    # plt.plot(data_buy.index, data_buy['price_native'], label='Buy Order')\n",
    "    # plt.plot(data_sell.index, data_sell['price_native'], label='Sell Order')\n",
    "\n",
    "    # # Set the plot title and axis labels\n",
    "    # plt.title(f'{name} Returns by Buy/Sell Order')\n",
    "    # plt.xlabel('Date')\n",
    "    # plt.ylabel('Price')\n",
    "\n",
    "    # # Add a legend to the plot\n",
    "    # plt.legend()\n",
    "\n",
    "    # # Show the plot\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    ###Ad Hoc alternative price and returns computation - as ration of tokens traded\n",
    "    # df[\"price_native_derived\"] = df[\"total_native\"]/df[\"amount_token\"]\n",
    "    # # plt.plot(df[\"price_native_derived\"])\n",
    "    # # plt.show()\n",
    "\n",
    "    # # print(name)\n",
    "    # # print(df[\"price_native_derived\"].min())\n",
    "    # # q = df[\"price_native_derived\"].quantile([0.01, 0.05, 0.25, 0.5, 0.75, 0.95, 0.99])\n",
    "    # # print(q)\n",
    "    # # print(df[\"price_native_derived\"].max(), \"\\n\")\n",
    "\n",
    "    # add_returns(df, price_col='price_native_derived', ret_col='return_native_derived', log_ret_col='log_return_native_derived')\n",
    "\n",
    "    # lower_limit = np.percentile(df[\"return_native_derived\"], 0.01)*1.1\n",
    "    # higher_limit = np.percentile(df[\"return_native_derived\"], 0.99)*1.1\n",
    "\n",
    "    # plt.hist(df[\"return_native_derived\"], bins=600)\n",
    "    # #plt.xlim(-0.5, 0.5)\n",
    "    # plt.show() ##it seemed good, but it probably wont help me, there is still a peak in the far\n",
    "\n",
    "\n",
    "print(table)\n",
    "\n",
    "#table.to_csv(r\"D:\\Dokumenty\\Vejška\\Magisterské studium\\DIPLOMKA\\Code_and_Data\\Data_scraping\\DEX_data_scraper\\complete_data\\raw_prices_overview.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def rescale_price_to_time_interval(df, time_interval, price_columns=('price_native', 'price_USD'), rescale_method='close'):\n",
    "    \"\"\"\n",
    "    Rescales price column in a Pandas DataFrame to a specified time interval.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Original DataFrame containing price data.\n",
    "        time_interval (str): Time interval for rescaling. Allowed values are '5Min', '15Min', or '1H' for 5 minutes,\n",
    "                            15 minutes, or 1 hour, respectively.\n",
    "        price_column (str, optional): Column name of the price data. Defaults to 'price_native'.\n",
    "        rescale_method (str, optional): Method for rescaling. Allowed values are 'high', 'low', or 'close' to rescale\n",
    "                                       to the highest, lowest, or closing price, respectively. Defaults to 'close'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Rescaled DataFrame with time intervals and rescaled price column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Resample to desired time interval and select rescale method\n",
    "    if rescale_method == 'high':\n",
    "        rescaled_price_native = df[price_columns[0]].resample(time_interval).max()\n",
    "        rescaled_price_USD = df[price_columns[1]].resample(time_interval).max()\n",
    "    elif rescale_method == 'low':\n",
    "        rescaled_price_native = df[price_columns[0]].resample(time_interval).min()\n",
    "        rescaled_price_USD = df[price_columns[1]].resample(time_interval).min()\n",
    "    elif rescale_method == 'close':\n",
    "        rescaled_price_native = df[price_columns[0]].resample(time_interval).last()\n",
    "        rescaled_price_USD = df[price_columns[1]].resample(time_interval).last()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid rescale_method. Allowed values are 'high', 'low', or 'close'.\")\n",
    "\n",
    "    # Forward fill missing values with last valid value\n",
    "    rescaled_price_native = rescaled_price_native.ffill()\n",
    "    rescaled_price_USD = rescaled_price_USD.ffill()\n",
    "\n",
    "    # Create rescaled DataFrame\n",
    "    rescaled_df = pd.DataFrame(rescaled_price_native)\n",
    "    rescaled_df[\"price_USD\"] = rescaled_price_USD\n",
    "\n",
    "    return rescaled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_realized_variance(df, df_rescaled = None, price_col = \"price_native\", mode=1, rescale_factor=None, aggregation_period=None):\n",
    "    \"\"\"\n",
    "    Calculate Realized Variance (RV) based on different modes.\n",
    "\n",
    "    Args:\n",
    "        prices (pd.Series): Series of prices with datetime index.\n",
    "        returns (pd.Series, optional): Series of returns. If not provided, it will be calculated from prices. \n",
    "                                       Default is None.\n",
    "        mode (int, optional): Mode to calculate RV. \n",
    "                               1 - Calculate RV on base price data, disregarding rescaling.\n",
    "                               2 - Calculate RV on base price data, disregarding rescaling, \n",
    "                                   but divide it by a number of observations in each block.\n",
    "                               3 - Sum returns over longer periods based on 5-min aggregated returns. \n",
    "                                   Default is 1.\n",
    "        rescale_factor (int or None, optional): Rescaling factor for price data. \n",
    "                                                Required for mode 1 and mode 2. Default is None.\n",
    "        aggregation_period (str or None, optional): Aggregation period for returns. \n",
    "                                                     Required for mode 3. Default is None.\n",
    "\n",
    "    Returns:\n",
    "        Dataframe with Realized Variance\n",
    "    \"\"\"\n",
    "    if mode == 1:\n",
    "        # Calculate simple returns\n",
    "        df[\"return_native\"] = df[price_col].pct_change()        \n",
    "        rv = (df[\"return_native\"]**2).resample(\"5Min\").sum()\n",
    "\n",
    "    elif mode == 2:\n",
    "        df[\"return_native\"] = df[price_col].pct_change()    \n",
    "        count = df[\"return_native\"].resample(\"5Min\").count()\n",
    "        count = count.replace(0, 1)    \n",
    "        rv = (df[\"return_native\"]**2).resample(\"5Min\").sum()/count\n",
    "\n",
    "    elif mode == 3:\n",
    "        rv = (df_rescaled[\"return_native\"]**2).resample(\"1H\").sum()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid mode value. Please select mode 1, mode 2, or mode 3.\")\n",
    "\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AGIX-WETH\n",
      "HEX-WETH\n",
      "OPTIMUS-WETH\n",
      "SHIB-WETH\n",
      "SHIK-WETH\n",
      "BabyDoge-WBNB\n",
      "FLOKI-WBNB\n",
      "INJ-WBNB\n",
      "MBOX-WBNB\n",
      "VOLT-WBNB\n",
      "            name observations          mean    median       StD           CV  \\\n",
      "0      AGIX-WETH         5416 -4.846103e-05  0.000000  0.004083   -84.251413   \n",
      "1       HEX-WETH         4991  5.607362e-05  0.000199  0.012248   218.431288   \n",
      "2   OPTIMUS-WETH         4964  2.313354e-06  0.000000  0.011339  4901.335959   \n",
      "3      SHIB-WETH         5208 -9.094539e-07  0.000000  0.003550 -3903.486679   \n",
      "4      SHIK-WETH         5329 -4.358089e-05  0.000000  0.011283  -258.905109   \n",
      "5  BabyDoge-WBNB         4594  2.965869e-05  0.000000  0.003547   119.590287   \n",
      "6     FLOKI-WBNB         4388 -7.053557e-06  0.000000  0.003991  -565.765444   \n",
      "7       INJ-WBNB         5210  4.318333e-05  0.000000  0.004334   100.359493   \n",
      "8      MBOX-WBNB         4552 -2.042600e-06  0.000000  0.003127 -1530.865926   \n",
      "9      VOLT-WBNB         4715 -3.890263e-05  0.000000  0.003689   -94.826602   \n",
      "\n",
      "    maximum   minimum  skewness    kurtosis   jarque_bera        ADF  \n",
      "0  0.034690 -0.043198 -0.211168   13.237481  2.369148e+04 -32.650154  \n",
      "1  0.261426 -0.202094  3.847106  198.920754  7.994780e+06 -50.225062  \n",
      "2  0.154196 -0.081216  0.924270   21.571713  7.204535e+04 -64.153018  \n",
      "3  0.030364 -0.025481  0.158701    7.275701  3.988974e+03 -33.695914  \n",
      "4  0.141626 -0.112736  0.141023   26.042472  1.179119e+05 -21.224931  \n",
      "5  0.058492 -0.022052  1.398493   24.488643  8.988638e+04 -17.525204  \n",
      "6  0.024150 -0.042940 -0.350230    8.369004  5.360097e+03 -55.201015  \n",
      "7  0.050223 -0.030351  0.688801   12.558047  2.024390e+04 -30.501859  \n",
      "8  0.021206 -0.011170  0.166836    3.714184  1.178581e+02 -31.553097  \n",
      "9  0.025437 -0.040886 -0.529956    9.064031  7.444962e+03 -29.981720  \n"
     ]
    }
   ],
   "source": [
    "#loop through all 10 datasets\n",
    "table = None\n",
    "for pair in pair_list:\n",
    "    #geting name of current pair and folder location\n",
    "    name = pair[0]\n",
    "    print(name)\n",
    "    file_path = f\"D:/Dokumenty/Vejška/Magisterské studium/DIPLOMKA/Code_and_Data/Data_scraping/DEX_data_scraper/complete_data/{name}/{name}_complete.csv\"\n",
    "\n",
    "    #loading data without werbose printout, seting index\n",
    "    df = load_csv_to_df(file_path, verbose=False, name = name)\n",
    "    set_index(df, info=False)\n",
    "\n",
    "    df_rescaled = rescale_price_to_time_interval(df,time_interval = \"5Min\", rescale_method='close')\n",
    "\n",
    "    #print(df_rescaled.info())\n",
    "\n",
    "    #Price plots\n",
    "    #plot_price(df_rescaled, native=True, name=name, ax=None, plot_both=True)\n",
    "  \n",
    "    #adding retruns and plotting them\n",
    "    add_returns(df_rescaled, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    add_returns(df_rescaled, price_col='price_USD', ret_col='return_USD', log_ret_col='log_return_USD')\n",
    "    \n",
    "    # #plot_returns(df, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=True, bins=100)\n",
    "    # plot_returns(df_rescaled, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=False, bins=100)\n",
    "    # plot_returns(df_rescaled, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=True, bins=100)\n",
    "    # print_stats(df_rescaled, native=True, log=False, name=name)\n",
    "\n",
    "\n",
    "\n",
    "    ####################Test Statistics\n",
    "    ##print Jarque-Bera test -- H0: Skewness and Kurtosis match Normal distribution - strongly rejected\n",
    "    #print(stats.jarque_bera(df_rescaled.return_native))\n",
    "\n",
    "    test_stationarity = False\n",
    "    if test_stationarity:\n",
    "        ##print ADF (Augment Dickey-Fuller) -- H0: there is a unit root present (i.e. series is not stationary) - strongly rejected\n",
    "        result = adfuller(df_rescaled.return_native)\n",
    "        print('ADF Statistic: %f' % result[0], 'p-value: %f' % result[1])\n",
    "\n",
    "        ##KPSS test for stationarity: H0 - The time series is trend stationary., H1 - not trend stationary. Failed to reject in all cases\n",
    "        kpss_result = kpss(df_rescaled.return_native)\n",
    "        print(f\"KPSS test statistic {kpss_result[0]} with p-value {kpss_result[1]} on {kpss_result[2]} lags.\")\n",
    "\n",
    "        ##PP test for stationarity: H - there is a unit root, H1: no unit root. Strongly rejected in all cases.\n",
    "        pp_result = pp = PhillipsPerron(df_rescaled.return_native)\n",
    "        print(pp.summary().as_text())\n",
    "\n",
    "    test_autocorr = False\n",
    "    if test_autocorr:\n",
    "        ##Durbin-Watson test of autocorrelation - H0 no autocorrelation, values around 2, H1 autocorrelation, close to 0 - positive, close to 4 negative\n",
    "        ##rule of thumb: test statistic values between the range of 1.5 and 2.5 are considered normal\n",
    "        print(durbin_watson(df_rescaled.return_native))#, \"  \", durbin_watson(df_rescaled.log_return_native)) #expect not to reject H0\n",
    "        print(durbin_watson(np.abs(df_rescaled.return_native)))#, \"  \", durbin_watson(np.abs(df_rescaled.return_native))) #expect to reject H0 for absolute values of returns - other form of clustering of Realized Variance\n",
    "        print(\"\\n\") #note on comparison of returns and logreturns:almost exactly same results\n",
    "\n",
    "        ##Ljung-Box test for autocorrelation - H0 residuals are independently distributed (no autocorr.). H1 residuals exhibit a serial correlation\n",
    "        # print(sm.stats.acorr_ljungbox(df_rescaled.return_native, lags=[15], return_df=True)) #result - rejected H0 of autocorr in all cases - I would like to see no autocorr here\n",
    "        # print(sm.stats.acorr_ljungbox(np.abs(df_rescaled.return_native), lags=[15], return_df=True)) #result - rejected H0 of autocorr in all cases (except BabyDoge, weird) - at least more strongly\n",
    "        # print(\"\\n\")\n",
    "\n",
    "        ##Breusch-Godfrey tests for autocorrelation - H0 There is no autocorrelation at any order less than or equal to p. H1 There exists autocorrelation at some order less than or equal to p.\n",
    "        #print(sm.stats.diagnostic.acorr_breusch_godfrey(df_rescaled.return_native))\n",
    "        #print(\"\\n\") #does not work, will not be using this one\n",
    "\n",
    "    table = get_overview_table(df_rescaled, name=name, column = \"return_native\", table = table)\n",
    "    #table = get_overview_table(df_rescaled, name=name, column = \"log_return_native\", table = table)\n",
    "    \n",
    "\n",
    "\n",
    "    ##############ACF and PACF\n",
    "    plot_acf_switch = False\n",
    "    if plot_acf_switch:\n",
    "        plot_acf_pacf(np.abs(df_rescaled.return_native), lags=35, figsize=(9, 8))\n",
    "\n",
    "\n",
    "\n",
    "    ##############GARCH models\n",
    "\n",
    "    GARCH_switch = False\n",
    "    if GARCH_switch:\n",
    "        am = arch_model(y = df_rescaled.return_native, mean='Constant', vol=\"GARCH\",\n",
    "                        p=1, o=0, q=1, rescale=True)\n",
    "        res = am.fit()\n",
    "        print(res.summary())\n",
    "\n",
    "    GJRGARCH_switch = False\n",
    "    if GJRGARCH_switch:\n",
    "        am = arch_model(y = df_rescaled.return_native, mean='Constant', vol=\"GARCH\", dist = \"normal\",\n",
    "                        p=1, o=1, q=1, rescale=True)\n",
    "        res = am.fit()\n",
    "        print(res.summary())\n",
    "\n",
    "    EGARCH_switch = False\n",
    "    if EGARCH_switch:\n",
    "        am = arch_model(y = df_rescaled.return_native, mean='Constant', vol=\"EGARCH\", dist = \"normal\",\n",
    "                        p=1, o=1, q=1, rescale=True)\n",
    "        res = am.fit()\n",
    "        print(res.summary())\n",
    "\n",
    "    # EGARCH_switch_alternative = False\n",
    "    # if EGARCH_switch_alternative:\n",
    "    #     # am = arch_model(y = df_rescaled.return_native, mean='Constant', vol=\"EGARCH\", dist = \"normal\",\n",
    "    #     #                 p=1, o=1, q=1, rescale=True)\n",
    "    #     #am = EGARCH(p=1, o=1, q=1)\n",
    "    #     am = ConstantMean(y = df_rescaled.return_native, rescale = True)\n",
    "    #     am.volatility = EGARCH(p=1, o=1, q=1)\n",
    "    #     am.distribution = Normal()\n",
    "    #     res = am.fit()\n",
    "    #     print(res.summary())\n",
    "                    \n",
    "\n",
    "\n",
    "    ### Calculate RV\n",
    "    # rv_1 = calculate_realized_variance(df = df, df_rescaled = df_rescaled, price_col = \"price_native\", mode=1)\n",
    "    # rv_2 = calculate_realized_variance(df = df, df_rescaled = df_rescaled, price_col = \"price_native\", mode=2)\n",
    "    # rv_3 = calculate_realized_variance(df = df, df_rescaled = df_rescaled, price_col = \"price_native\", mode=3)\n",
    "    # # print(rv)\n",
    "    # # rv.plot()\n",
    "    # # plt.show()\n",
    "\n",
    "    # # Create subplots with 1 row and 3 columns\n",
    "    # fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    # # Plot rv_1 on the first subplot\n",
    "    # rv_1.plot(ax=axes[0])\n",
    "    # axes[0].set_title('RV_mode_1')\n",
    "\n",
    "    # # Plot rv_2 on the second subplot\n",
    "    # rv_2.plot(ax=axes[1])\n",
    "    # axes[1].set_title('RV_mode_2')\n",
    "\n",
    "    # # Plot rv_3 on the third subplot\n",
    "    # rv_3.plot(ax=axes[2])\n",
    "    # axes[2].set_title('RV_mode_3')\n",
    "\n",
    "    # # Set the overall title for the plot\n",
    "    # plt.suptitle(name + ' Realized Variance')\n",
    "\n",
    "    # # Display the plot\n",
    "    # plt.show()\n",
    "\n",
    "    # #Printing the DW test: close to 2 - no autocorrelation, outside of (1.5, 2.5) - autocorrelation\n",
    "    # #The square root is the Realized Volatility -- this should be autocorrelated acording to my text\n",
    "    # print(\"mode_1: \", durbin_watson(rv_1), \"sqrt: \", durbin_watson(np.sqrt(rv_1)))\n",
    "    # print(\"mode_2: \", durbin_watson(rv_2), \"sqrt: \", durbin_watson(np.sqrt(rv_2)))\n",
    "    # print(\"mode_3: \", durbin_watson(rv_3), \"sqrt: \", durbin_watson(np.sqrt(rv_3)))\n",
    "\n",
    "print(table)    \n",
    "#table.to_csv(r\"D:\\Dokumenty\\Vejška\\Magisterské studium\\DIPLOMKA\\Code_and_Data\\Data_scraping\\DEX_data_scraper\\complete_data\\returns_5min.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "#plot 10 price series or 10 returns\n",
    "# Create a 4x3 grid of subplots\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 15))\n",
    "#fig.suptitle('Return Series', fontsize=18)\n",
    "# Lists for plot ordering in the grid\n",
    "row_indices = [0,0,0,1,1,2,2,2,3,3]\n",
    "col_indices = [0,1,2,0,1,0,1,2,0,1]\n",
    "price_labels = [\"WETH\",\"WETH\",\"WETH\",\"WETH\",\"WETH\",\"WBNB\",\"WBNB\",\"WBNB\",\"WBNB\",\"WBNB\"]\n",
    "\n",
    "\n",
    "# Loop through each .csv file and plot on the corresponding subplot\n",
    "i=0\n",
    "for pair in pair_list:\n",
    "    #geting name of current pair and folder location\n",
    "    name = pair[0]\n",
    "    print(name)\n",
    "    file_path = f\"D:/Dokumenty/Vejška/Magisterské studium/DIPLOMKA/Code_and_Data/Data_scraping/DEX_data_scraper/complete_data/{name}/{name}_complete.csv\"\n",
    "\n",
    "    #loading data without werbose printout, seting index\n",
    "    df = load_csv_to_df(file_path, verbose=False, name = name)\n",
    "    set_index(df, info=False)\n",
    "\n",
    "    df_rescaled = rescale_price_to_time_interval(df,time_interval = \"5Min\", rescale_method='close')\n",
    "    add_returns(df_rescaled, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    add_returns(df_rescaled, price_col='price_USD', ret_col='return_USD', log_ret_col='log_return_USD')\n",
    "    \n",
    "    ###Extract the plotted series - here to switch between what is beeing plotted from which df\n",
    "    #plotted_series = df[\"price_native\"]\n",
    "    # add_returns(df, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    # plotted_series = df[\"return_native\"]\n",
    "    plotted_series = df_rescaled['return_native']\n",
    "    \n",
    "    # Compute the row and column index of the subplot\n",
    "    row_idx = row_indices[i]\n",
    "    col_idx = col_indices[i]\n",
    "    \n",
    "    ###Plot the price series on the subplot - here switch between time series plot and histogram\n",
    "    hist = True\n",
    "\n",
    "    if not hist:\n",
    "        axes[row_idx, col_idx].plot(plotted_series)\n",
    "        #axes[row_idx, col_idx].xaxis.set_major_locator(MultipleLocator(base=3.2))\n",
    "        #axes[row_idx, col_idx].xaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "        axes[row_idx, col_idx].xaxis.set_major_locator(LinearLocator(numticks=5)) #still not ideal tick locator, but maybe the best. It competes with the MultipleLocator.\n",
    "        axes[row_idx, col_idx].xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "\n",
    "        #axes[row_idx, col_idx].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        #axes[row_idx, col_idx].set_xlabel('Time')\n",
    "        #axes[row_idx, col_idx].set_ylabel(f'Price in {price_labels[i]}')\n",
    "        axes[row_idx, col_idx].set_ylabel(f'Returns')        \n",
    "    else:\n",
    "        axes[row_idx, col_idx].hist(plotted_series, bins = 55)\n",
    "\n",
    "\n",
    "        axes[row_idx, col_idx].set_ylabel(f'Returns count')\n",
    "\n",
    "\n",
    "    axes[row_idx, col_idx].set_title(f'{name}')\n",
    "    i+=1\n",
    "fig.delaxes(axes[1][2])\n",
    "fig.delaxes(axes[3][2])\n",
    "#fig.suptitle('Returns - tick to tick')\n",
    "# Add padding between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Set title to the figure\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "##Ad-Hoc plotting of separate buy/sell orders\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "#plot 10 price series or 10 returns\n",
    "# Create a 4x3 grid of subplots\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 15))\n",
    "#fig.suptitle('Return Series', fontsize=18)\n",
    "# Lists for plot ordering in the grid\n",
    "row_indices = [0,0,0,1,1,2,2,2,3,3]\n",
    "col_indices = [0,1,2,0,1,0,1,2,0,1]\n",
    "price_labels = [\"WETH\",\"WETH\",\"WETH\",\"WETH\",\"WETH\",\"WBNB\",\"WBNB\",\"WBNB\",\"WBNB\",\"WBNB\"]\n",
    "\n",
    "\n",
    "# Loop through each .csv file and plot on the corresponding subplot\n",
    "i=0\n",
    "for pair in pair_list:\n",
    "    #geting name of current pair and folder location\n",
    "    name = pair[0]\n",
    "    print(name)\n",
    "    file_path = f\"D:/Dokumenty/Vejška/Magisterské studium/DIPLOMKA/Code_and_Data/Data_scraping/DEX_data_scraper/complete_data/{name}/{name}_complete.csv\"\n",
    "\n",
    "    #loading data without werbose printout, seting index\n",
    "    df = load_csv_to_df(file_path, verbose=False, name = name)\n",
    "    set_index(df, info=False)\n",
    "\n",
    "    df_rescaled = rescale_price_to_time_interval(df,time_interval = \"5Min\", rescale_method='close')\n",
    "    add_returns(df_rescaled, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    add_returns(df_rescaled, price_col='price_USD', ret_col='return_USD', log_ret_col='log_return_USD')\n",
    "\n",
    "    data_buy = df[df['buy_order'] == True]\n",
    "    data_sell = df[df['buy_order'] == False]\n",
    "\n",
    "    # Compute the row and column index of the subplot\n",
    "    row_idx = row_indices[i]\n",
    "    col_idx = col_indices[i]\n",
    "\n",
    "    axes[row_idx, col_idx].plot(data_buy.index, data_buy['price_native'], label='Buy Order')\n",
    "    axes[row_idx, col_idx].plot(data_sell.index, data_sell['price_native'], label='Sell Order')\n",
    "    axes[row_idx, col_idx].set_title('Returns by Buy/Sell Order')\n",
    "    axes[row_idx, col_idx].set_xlabel('Date')\n",
    "    axes[row_idx, col_idx].set_ylabel('Price')\n",
    "    axes[row_idx, col_idx].legend()\n",
    "    \n",
    "    # ###Extract the plotted series - here to switch between what is beeing plotted from which df\n",
    "    # #plotted_series = df[\"price_native\"]\n",
    "    # # add_returns(df, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    # # plotted_series = df[\"return_native\"]\n",
    "    # plotted_series = df_rescaled['return_native']\n",
    "    \n",
    "    # # Compute the row and column index of the subplot\n",
    "    # row_idx = row_indices[i]\n",
    "    # col_idx = col_indices[i]\n",
    "    \n",
    "    # ###Plot the price series on the subplot - here switch between time series plot and histogram\n",
    "    # hist = True\n",
    "\n",
    "    # if not hist:\n",
    "    #     axes[row_idx, col_idx].plot(plotted_series)\n",
    "    #     #axes[row_idx, col_idx].xaxis.set_major_locator(MultipleLocator(base=3.2))\n",
    "    #     #axes[row_idx, col_idx].xaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    #     axes[row_idx, col_idx].xaxis.set_major_locator(LinearLocator(numticks=5)) #still not ideal tick locator, but maybe the best. It competes with the MultipleLocator.\n",
    "    #     axes[row_idx, col_idx].xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "\n",
    "    #     #axes[row_idx, col_idx].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "    #     #axes[row_idx, col_idx].set_xlabel('Time')\n",
    "    #     #axes[row_idx, col_idx].set_ylabel(f'Price in {price_labels[i]}')\n",
    "    #     axes[row_idx, col_idx].set_ylabel(f'Returns')        \n",
    "    # else:\n",
    "    #     axes[row_idx, col_idx].hist(plotted_series, bins = 55)\n",
    "\n",
    "\n",
    "    #     axes[row_idx, col_idx].set_ylabel(f'Returns count')\n",
    "\n",
    "\n",
    "    axes[row_idx, col_idx].set_title(f'{name}')\n",
    "    i+=1\n",
    "fig.delaxes(axes[1][2])\n",
    "fig.delaxes(axes[3][2])\n",
    "#fig.suptitle('Returns - tick to tick')\n",
    "# Add padding between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Set title to the figure\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'false'\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "##Ad-Hoc plotting of separate buy/sell histograms and ACF on aggregated data\n",
    "\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.ticker import LinearLocator\n",
    "\n",
    "#plot 10 price series or 10 returns\n",
    "# Create a 4x3 grid of subplots\n",
    "# fig, axes = plt.subplots(4, 3, figsize=(15, 15))\n",
    "#fig.suptitle('Return Series', fontsize=18)\n",
    "# Lists for plot ordering in the grid\n",
    "row_indices = [0,0,0,1,1,2,2,2,3,3]\n",
    "col_indices = [0,1,2,0,1,0,1,2,0,1]\n",
    "price_labels = [\"WETH\",\"WETH\",\"WETH\",\"WETH\",\"WETH\",\"WBNB\",\"WBNB\",\"WBNB\",\"WBNB\",\"WBNB\"]\n",
    "\n",
    "\n",
    "# Loop through each .csv file and plot on the corresponding subplot\n",
    "i=0\n",
    "for pair in pair_list:\n",
    "    #geting name of current pair and folder location\n",
    "    name = pair[0]\n",
    "    print(name)\n",
    "    file_path = f\"D:/Dokumenty/Vejška/Magisterské studium/DIPLOMKA/Code_and_Data/Data_scraping/DEX_data_scraper/complete_data/{name}/{name}_complete.csv\"\n",
    "\n",
    "    #loading data without werbose printout, seting index\n",
    "    df = load_csv_to_df(file_path, verbose=False, name = name)\n",
    "    set_index(df, info=False)\n",
    "\n",
    "    df_rescaled = rescale_price_to_time_interval(df,time_interval = \"5Min\", rescale_method='close')\n",
    "    add_returns(df_rescaled, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    add_returns(df_rescaled, price_col='price_USD', ret_col='return_USD', log_ret_col='log_return_USD')\n",
    "\n",
    "    data_buy = df[df['buy_order'] == True]\n",
    "    data_sell = df[df['buy_order'] == False]\n",
    "\n",
    "    data_buy_rescaled = rescale_price_to_time_interval(data_buy,time_interval = \"5Min\", rescale_method='close')\n",
    "    data_sell_rescaled = rescale_price_to_time_interval(data_sell,time_interval = \"5Min\", rescale_method='close')\n",
    "\n",
    "    add_returns(data_buy_rescaled, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    add_returns(data_sell_rescaled, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "\n",
    "    # #plot buy/sell return histogram\n",
    "    # plot_returns(data_sell_rescaled, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=True, bins=100)\n",
    "    # #conclusion from plotting separate histograms 5/4/2023: It helps, there is not the bid-ask spread. But a problem\n",
    "    # #would probably be in skewness - assymetric distribution...maybe, not sure\n",
    "\n",
    "    #plot buy/sell return ACF\n",
    "    plot_acf_pacf(np.abs(df_rescaled.return_native), lags=35, figsize=(9, 8))\n",
    "\n",
    "    \n",
    "\n",
    "    # Compute the row and column index of the subplot\n",
    "    # row_idx = row_indices[i]\n",
    "    # col_idx = col_indices[i]\n",
    "\n",
    "    # axes[row_idx, col_idx].plot(data_buy.index, data_buy['price_native'], label='Buy Order')\n",
    "    # axes[row_idx, col_idx].plot(data_sell.index, data_sell['price_native'], label='Sell Order')\n",
    "    # axes[row_idx, col_idx].set_title('Returns by Buy/Sell Order')\n",
    "    # axes[row_idx, col_idx].set_xlabel('Date')\n",
    "    # axes[row_idx, col_idx].set_ylabel('Price')\n",
    "    # axes[row_idx, col_idx].legend()\n",
    "    \n",
    "    # ###Extract the plotted series - here to switch between what is beeing plotted from which df\n",
    "    # #plotted_series = df[\"price_native\"]\n",
    "    # # add_returns(df, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    # # plotted_series = df[\"return_native\"]\n",
    "    # plotted_series = df_rescaled['return_native']\n",
    "    \n",
    "    # # Compute the row and column index of the subplot\n",
    "    # row_idx = row_indices[i]\n",
    "    # col_idx = col_indices[i]\n",
    "    \n",
    "    # ###Plot the price series on the subplot - here switch between time series plot and histogram\n",
    "    # hist = True\n",
    "\n",
    "    # if not hist:\n",
    "    #     axes[row_idx, col_idx].plot(plotted_series)\n",
    "    #     #axes[row_idx, col_idx].xaxis.set_major_locator(MultipleLocator(base=3.2))\n",
    "    #     #axes[row_idx, col_idx].xaxis.set_major_locator(MaxNLocator(nbins=6))\n",
    "    #     axes[row_idx, col_idx].xaxis.set_major_locator(LinearLocator(numticks=5)) #still not ideal tick locator, but maybe the best. It competes with the MultipleLocator.\n",
    "    #     axes[row_idx, col_idx].xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
    "\n",
    "    #     #axes[row_idx, col_idx].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "    #     #axes[row_idx, col_idx].set_xlabel('Time')\n",
    "    #     #axes[row_idx, col_idx].set_ylabel(f'Price in {price_labels[i]}')\n",
    "    #     axes[row_idx, col_idx].set_ylabel(f'Returns')        \n",
    "    # else:\n",
    "    #     axes[row_idx, col_idx].hist(plotted_series, bins = 55)\n",
    "\n",
    "\n",
    "    #     axes[row_idx, col_idx].set_ylabel(f'Returns count')\n",
    "\n",
    "\n",
    "#     axes[row_idx, col_idx].set_title(f'{name}')\n",
    "#     i+=1\n",
    "# fig.delaxes(axes[1][2])\n",
    "# fig.delaxes(axes[3][2])\n",
    "#fig.suptitle('Returns - tick to tick')\n",
    "# Add padding between subplots\n",
    "# plt.tight_layout()\n",
    "\n",
    "# Set title to the figure\n",
    "\n",
    "\n",
    "# Display the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to get string stars for the table\n",
    "def get_p_stars(x):\n",
    "    if x > 0.1:\n",
    "        return \" \"\n",
    "    elif x > 0.05:\n",
    "        return \"^*\"\n",
    "    elif x > 0.01:\n",
    "        return \"^{**}\"\n",
    "    else:\n",
    "        return \"^{***}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~---AGIX-WETH---~~~\n",
      "EGARCH\n",
      "                params       pvalues pvalues\n",
      "mu        5.847577e-08  9.918294e-01        \n",
      "omega     4.212189e-02  9.135357e-01        \n",
      "alpha[1]  2.081932e-01  6.667721e-01        \n",
      "gamma[1]  3.359470e-02  6.738407e-01        \n",
      "beta[1]   1.000000e+00  0.000000e+00       *\n",
      "eta       2.050000e+00  3.889823e-14       *\n",
      "lambda   -7.654665e-03  1.757511e-02       *\n",
      "\n",
      "\n",
      "~~~---HEX-WETH---~~~\n",
      "EGARCH\n",
      "            params       pvalues pvalues\n",
      "mu       -0.014775  1.194292e-01        \n",
      "omega     0.335225  3.562100e-01        \n",
      "alpha[1]  0.632708  2.547004e-02       *\n",
      "gamma[1] -0.305137  4.055054e-02       *\n",
      "beta[1]   0.783751  1.485594e-29       *\n",
      "eta       2.192005  3.700169e-38       *\n",
      "lambda   -0.107515  1.842668e-12       *\n",
      "\n",
      "\n",
      "~~~---OPTIMUS-WETH---~~~\n",
      "EGARCH\n",
      "            params        pvalues pvalues\n",
      "mu       -0.035113   5.566197e-03       *\n",
      "omega     0.549547   2.224493e-01        \n",
      "alpha[1]  0.696773   1.217808e-01        \n",
      "gamma[1] -0.054452   3.526922e-01        \n",
      "beta[1]   0.920901   0.000000e+00       *\n",
      "eta       2.050000  1.931085e-233       *\n",
      "lambda   -0.045753   1.965067e-04       *\n",
      "\n",
      "\n",
      "~~~---SHIB-WETH---~~~\n",
      "EGARCH\n",
      "            params        pvalues pvalues\n",
      "mu       -0.000346   8.673923e-01        \n",
      "omega     0.198626   8.821824e-01        \n",
      "alpha[1]  0.444551   7.833702e-01        \n",
      "gamma[1] -0.037634   7.771526e-01        \n",
      "beta[1]   1.000000  5.316212e-254       *\n",
      "eta       2.050000   1.405696e-06       *\n",
      "lambda    0.002361   8.036157e-01        \n",
      "\n",
      "\n",
      "~~~---SHIK-WETH---~~~\n",
      "EGARCH\n",
      "            params        pvalues pvalues\n",
      "mu       -0.000323   7.983190e-01        \n",
      "omega     0.050346   4.658477e-01        \n",
      "alpha[1]  0.165628   7.220170e-02        \n",
      "gamma[1] -0.092183   8.826610e-02        \n",
      "beta[1]   1.000000   0.000000e+00       *\n",
      "eta       2.050000  5.122624e-235       *\n",
      "lambda   -0.012697   4.193691e-02       *\n",
      "\n",
      "\n",
      "~~~---BabyDoge-WBNB---~~~\n",
      "EGARCH\n",
      "             params       pvalues pvalues\n",
      "mu        -0.050521  1.074086e-05       *\n",
      "omega     -1.035116  4.874971e-09       *\n",
      "alpha[1]   0.120256  2.492020e-03       *\n",
      "gamma[1]  -0.303473  3.450716e-06       *\n",
      "beta[1]    0.496416  5.398375e-09       *\n",
      "eta       13.979381  2.773148e-02       *\n",
      "lambda    -0.105351  2.601654e-05       *\n",
      "\n",
      "\n",
      "~~~---FLOKI-WBNB---~~~\n",
      "EGARCH\n",
      "             params       pvalues pvalues\n",
      "mu        -0.003364  6.631578e-01        \n",
      "omega     -0.270958  1.101134e-02       *\n",
      "alpha[1]   0.123993  2.383850e-04       *\n",
      "gamma[1]   0.006415  8.151332e-01        \n",
      "beta[1]    0.851000  1.647036e-49       *\n",
      "eta       10.583294  9.453603e-06       *\n",
      "lambda    -0.028418  5.927941e-02        \n",
      "\n",
      "\n",
      "~~~---INJ-WBNB---~~~\n",
      "EGARCH\n",
      "            params        pvalues pvalues\n",
      "mu        0.001710   7.070854e-01        \n",
      "omega     0.355402   6.284524e-01        \n",
      "alpha[1]  0.611357   5.302285e-01        \n",
      "gamma[1]  0.244058   3.187427e-01        \n",
      "beta[1]   0.982182  7.006847e-226       *\n",
      "eta       2.050000   1.664388e-80       *\n",
      "lambda    0.013613   2.250685e-01        \n",
      "\n",
      "\n",
      "~~~---MBOX-WBNB---~~~\n",
      "EGARCH\n",
      "            params       pvalues pvalues\n",
      "mu        0.219898  3.375317e-02       *\n",
      "omega     1.440742  2.981052e-01        \n",
      "alpha[1]  1.048756  2.959148e-01        \n",
      "gamma[1]  1.431755  3.371015e-01        \n",
      "beta[1]   0.803512  1.588936e-14       *\n",
      "eta       2.050000  1.042877e-94       *\n",
      "lambda    0.100854  2.314515e-03       *\n",
      "\n",
      "\n",
      "~~~---VOLT-WBNB---~~~\n",
      "EGARCH\n",
      "            params   pvalues pvalues\n",
      "mu       -0.000315  0.872047        \n",
      "omega     0.415389  0.061932        \n",
      "alpha[1]  0.748157  0.009388       *\n",
      "gamma[1]  0.024179  0.490875        \n",
      "beta[1]   1.000000  0.000000       *\n",
      "eta       2.050000  0.000000       *\n",
      "lambda   -0.007878  0.546424        \n",
      "\n",
      "\n",
      "      Token            mu         omega        alpha         gamma  \\\n",
      "0      AGIX          0.0         0.042        0.208         0.034    \n",
      "1       HEX       -0.015         0.335    0.633^{**}   -0.305^{**}   \n",
      "2   OPTIMUS  -0.035^{***}         0.55        0.697        -0.054    \n",
      "3      SHIB         -0.0         0.199        0.445        -0.038    \n",
      "4      SHIK         -0.0          0.05       0.166^*      -0.092^*   \n",
      "5  BabyDoge  -0.051^{***}  -1.035^{***}   0.12^{***}  -0.303^{***}   \n",
      "6     FLOKI       -0.003    -0.271^{**}  0.124^{***}        0.006    \n",
      "7       INJ        0.002         0.355        0.611         0.244    \n",
      "8      MBOX     0.22^{**}        1.441        1.049         1.432    \n",
      "9      VOLT         -0.0        0.415^*  0.748^{***}        0.024    \n",
      "\n",
      "          beta  alpha+beta  LogLikelihood      AIC      BIC  \n",
      "0    1.0^{***}       0.242          836.2  -1658.4  -1612.3  \n",
      "1  0.784^{***}       0.328        -4103.6   8221.2   8266.9  \n",
      "2  0.921^{***}       0.642        -5664.4  11342.8  11388.4  \n",
      "3    1.0^{***}       0.407         -577.9   1169.8   1215.7  \n",
      "4    1.0^{***}       0.073        -3556.6   7127.2   7173.3  \n",
      "5  0.496^{***}      -0.183        -1569.7   3153.4   3198.4  \n",
      "6  0.851^{***}       0.130        -2035.5   4085.1   4129.8  \n",
      "7  0.982^{***}       0.855        -1729.0   3471.9   3517.8  \n",
      "8  0.804^{***}       2.481       -11254.7  22523.5  22568.4  \n",
      "9    1.0^{***}       0.772         -507.4   1028.9   1074.1  \n"
     ]
    }
   ],
   "source": [
    "##Table for tests/GARCH models\n",
    "#loop through all 10 datasets\n",
    "table = None\n",
    "GARCH_table = []\n",
    "GJR_GARCH_table = []\n",
    "EGARCH_table = []\n",
    "for pair in pair_list:\n",
    "    #geting name of current pair and folder location\n",
    "    name = pair[0]\n",
    "    print(f\"~~~---{name}---~~~\")\n",
    "    file_path = f\"D:/Dokumenty/Vejška/Magisterské studium/DIPLOMKA/Code_and_Data/Data_scraping/DEX_data_scraper/complete_data/{name}/{name}_complete.csv\"\n",
    "\n",
    "    #loading data without werbose printout, seting index\n",
    "    df = load_csv_to_df(file_path, verbose=False, name = name)\n",
    "    set_index(df, info=False)\n",
    "\n",
    "    df_rescaled = rescale_price_to_time_interval(df,time_interval = \"5Min\", rescale_method='close')\n",
    "\n",
    "    #print(df_rescaled.info())\n",
    "\n",
    "    #Price plots\n",
    "    #plot_price(df_rescaled, native=True, name=name, ax=None, plot_both=True)\n",
    "  \n",
    "    #adding retruns and plotting them\n",
    "    add_returns(df_rescaled, price_col='price_native', ret_col='return_native', log_ret_col='log_return_native')\n",
    "    add_returns(df_rescaled, price_col='price_USD', ret_col='return_USD', log_ret_col='log_return_USD')\n",
    "    \n",
    "    # #plot_returns(df, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=True, bins=100)\n",
    "    # plot_returns(df_rescaled, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=False, bins=100)\n",
    "    # plot_returns(df_rescaled, native=True, log=False, name=name, lower_percentile=0, higher_percentile=100, histogram=True, bins=100)\n",
    "    # print_stats(df_rescaled, native=True, log=False, name=name)\n",
    "\n",
    "\n",
    "\n",
    "    ####################Test Statistics\n",
    "    ##print Jarque-Bera test -- H0: Skewness and Kurtosis match Normal distribution - strongly rejected\n",
    "    #print(stats.jarque_bera(df_rescaled.return_native))\n",
    "\n",
    "    test_stationarity = False\n",
    "    if test_stationarity:\n",
    "        ##print ADF (Augment Dickey-Fuller) -- H0: there is a unit root present (i.e. series is not stationary) - strongly rejected\n",
    "        result = adfuller(df_rescaled.return_native, maxlag = 50, autolag=None)\n",
    "        print('ADF Statistic: %f' % result[0], 'p-value: %f' % result[1], 'lags: %f' % result[2])\n",
    "\n",
    "        ##KPSS test for stationarity: H0 - The time series is trend stationary., H1 - not trend stationary. Failed to reject in all cases\n",
    "        kpss_result = kpss(df_rescaled.return_native, nlags=50)\n",
    "        print(f\"KPSS test statistic {kpss_result[0]} with p-value {kpss_result[1]} on {kpss_result[2]} lags.\")\n",
    "\n",
    "        ##PP test for stationarity: H - there is a unit root, H1: no unit root. Strongly rejected in all cases.\n",
    "        pp_result = pp = PhillipsPerron(df_rescaled.return_native, lags = 50)\n",
    "        print(pp.summary().as_text())\n",
    "\n",
    "    test_autocorr = True\n",
    "    if test_autocorr:\n",
    "        DW_test = False\n",
    "        if DW_test:\n",
    "            ##Durbin-Watson test of autocorrelation - H0 no autocorrelation, values around 2, H1 autocorrelation, close to 0 - positive, close to 4 negative\n",
    "            ##rule of thumb: test statistic values between the range of 1.5 and 2.5 are considered normal\n",
    "            print(durbin_watson(df_rescaled.return_native))#, \"  \", durbin_watson(df_rescaled.log_return_native)) #expect not to reject H0\n",
    "            print(durbin_watson(np.abs(df_rescaled.return_native)))#, \"  \", durbin_watson(np.abs(df_rescaled.return_native))) #expect to reject H0 for absolute values of returns - other form of clustering of Realized Variance\n",
    "            print(\"\\n\") #note on comparison of returns and logreturns:almost exactly same results\n",
    "\n",
    "        Ljung_Box_test = False\n",
    "        if Ljung_Box_test:\n",
    "            ##Ljung-Box test for autocorrelation - H0 residuals are independently distributed (no autocorr.). H1 residuals exhibit a serial correlation\n",
    "            print(sm.stats.acorr_ljungbox(df_rescaled.log_return_native, lags=list(range(1, 16)), return_df=True)) #result - rejected H0 of autocorr in all cases - I would like to see no autocorr here\n",
    "            print(sm.stats.acorr_ljungbox(np.abs(df_rescaled.log_return_native), lags=list(range(1, 16)), return_df=True)) #result - rejected H0 of autocorr in all cases (except BabyDoge, weird) - at least more strongly\n",
    "            print(\"\\n\")\n",
    "\n",
    "        ##Breusch-Godfrey tests for autocorrelation - H0 There is no autocorrelation at any order less than or equal to p. H1 There exists autocorrelation at some order less than or equal to p.\n",
    "        #print(sm.stats.diagnostic.acorr_breusch_godfrey(df_rescaled.return_native))\n",
    "        #print(\"\\n\") #does not work, will not be using this one\n",
    "\n",
    "    table = get_overview_table(df_rescaled, name=name, column = \"return_native\", table = table)\n",
    "    #table = get_overview_table(df_rescaled, name=name, column = \"log_return_native\", table = table)\n",
    "    \n",
    "\n",
    "\n",
    "    ##############ACF and PACF\n",
    "    plot_acf_switch = False\n",
    "    if plot_acf_switch:\n",
    "        plot_acf_pacf(np.abs(df_rescaled.return_native), lags=35, figsize=(9, 8))\n",
    "\n",
    "\n",
    "\n",
    "    ##############GARCH models\n",
    "\n",
    "    GARCH_switch = False #https://arch.readthedocs.io/en/latest/univariate/introduction.html\n",
    "    if GARCH_switch:\n",
    "        am = arch_model(y = df_rescaled.return_native, mean='Constant', vol=\"GARCH\", dist='skewt', #'normal', 'studentst', 'skewt',\n",
    "                        p=1, o=0, q=1, rescale=True)\n",
    "        res = am.fit(disp=\"off\")\n",
    "        summary_short = pd.concat([res.params, res.pvalues, res.pvalues.apply(lambda x: '*' if x < 0.05 else ' ')], axis=1)\n",
    "        #summary_table_row_old = [name[:-5], round(res.params[0],3), get_p_stars(res.pvalues[0]), res.params[1], get_p_stars(res.pvalues[1]), res.params[2], get_p_stars(res.pvalues[2]), res.params[3], get_p_stars(res.pvalues[3]), res.params[2]+res.params[3], res.loglikelihood, res.aic, res.bic]\n",
    "        summary_table_row = [name[:-5], str(round(res.params[0],3))+get_p_stars(res.pvalues[0]), \n",
    "                             str(round(res.params[1],3))+get_p_stars(res.pvalues[1]),\n",
    "                             str(round(res.params[2],3))+get_p_stars(res.pvalues[2]),\n",
    "                             str(round(res.params[3],3))+get_p_stars(res.pvalues[3]),\n",
    "                             round(res.params[2]+res.params[3],3), \n",
    "                             round(res.loglikelihood,1),\n",
    "                             round(res.aic,1),\n",
    "                             round(res.bic,1)]\n",
    "        GARCH_table.append(summary_table_row)\n",
    "        print(\"GARCH\")\n",
    "        print(summary_short.iloc[0:])\n",
    "        #print(summary_table_row)\n",
    "        #print(res.summary())\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "#log of variants\n",
    "#GARCH 1,1 normal - 3* 7**        \n",
    "#GARCH p,q normal - seems worse  \n",
    "# \n",
    "#GARCH 1,1 studentst - 1/ 3* 6**    \n",
    "#GARCH p,q studentst - seems worse  \n",
    "#\n",
    "#GARCH 1,1 skewt - 3* 7**    \n",
    "#GARCH p,q skewt - seems worse ###conclusion - either normal or skewt\n",
    "\n",
    "    GJRGARCH_switch = False #https://arch.readthedocs.io/en/latest/univariate/univariate_volatility_modeling.html\n",
    "    if GJRGARCH_switch:\n",
    "        am = arch_model(y = df_rescaled.return_native, mean='Constant', vol=\"GARCH\", dist = \"skewt\",\n",
    "                        p=1, o=1, q=1, rescale=True)\n",
    "        res = am.fit(disp=\"off\")\n",
    "        summary_short = pd.concat([res.params, res.pvalues, res.pvalues.apply(lambda x: '*' if x < 0.05 else ' ')], axis=1)\n",
    "        #summary_table_row_old = [name[:-5], res.params[0], res.pvalues[0], res.params[1], res.pvalues[1], res.params[2], res.pvalues[2], res.params[3], res.pvalues[3], res.params[4], res.pvalues[4], res.rsquared, res.loglikelihood, res.aic, res.bic]\n",
    "        summary_table_row = [name[:-5], str(round(res.params[0],3))+get_p_stars(res.pvalues[0]), \n",
    "                        str(round(res.params[1],3))+get_p_stars(res.pvalues[1]),\n",
    "                        str(round(res.params[2],3))+get_p_stars(res.pvalues[2]),\n",
    "                        str(round(res.params[3],3))+get_p_stars(res.pvalues[3]),\n",
    "                        str(round(res.params[4],3))+get_p_stars(res.pvalues[4]),\n",
    "                        round(res.params[2]+res.params[3],3), \n",
    "                        round(res.loglikelihood,1),\n",
    "                        round(res.aic,1),\n",
    "                        round(res.bic,1)]\n",
    "        GJR_GARCH_table.append(summary_table_row)\n",
    "        print(\"GJR-GARCH\")\n",
    "        print(summary_short.iloc[0:])\n",
    "        #print(summary_table_row)\n",
    "        #print(res.summary())\n",
    "        print(\"\\n\")\n",
    "\n",
    "#log of variants\n",
    "#GJR-GARCH 1,1,1 normal - 1/ 4* 3** 2***           \n",
    "#GJR-GARCH p,o,q normal - seems worse\n",
    "#\n",
    "#GJR-GARCH 1,1,1 studentst - 1/ 5** 4***\n",
    "#GJR-GARCH p,o,q studentst - didnt try\n",
    "#\n",
    "#GJR-GARCH 1,1,1 skewt - 2* 4** 4***\n",
    "#GJR-GARCH p,o,q skewt - didnt try ###conclusion - here skewt seems as the best option\n",
    "\n",
    "    EGARCH_switch = True #https://arch.readthedocs.io/en/latest/univariate/generated/arch.univariate.EGARCH.html#arch.univariate.EGARCH\n",
    "    if EGARCH_switch:\n",
    "        am = arch_model(y = df_rescaled.return_native, mean='Constant', vol=\"EGARCH\", dist = \"skewt\",\n",
    "                        p=1, o=1, q=1, rescale=True)\n",
    "        res = am.fit(disp=\"off\")\n",
    "        summary_short = pd.concat([res.params, res.pvalues, res.pvalues.apply(lambda x: '*' if x < 0.05 else ' ')], axis=1)\n",
    "        #summary_table_row = [name[:-5], res.params[0], res.pvalues[0], res.params[1], res.pvalues[1], res.params[2], res.pvalues[2], res.params[3], res.pvalues[3], res.params[4], res.pvalues[4], res.rsquared, res.loglikelihood, res.aic, res.bic]\n",
    "        summary_table_row = [name[:-5], str(round(res.params[0],3))+get_p_stars(res.pvalues[0]), \n",
    "                        str(round(res.params[1],3))+get_p_stars(res.pvalues[1]),\n",
    "                        str(round(res.params[2],3))+get_p_stars(res.pvalues[2]),\n",
    "                        str(round(res.params[3],3))+get_p_stars(res.pvalues[3]),\n",
    "                        str(round(res.params[4],3))+get_p_stars(res.pvalues[4]),\n",
    "                        round(res.params[2]+res.params[3],3), \n",
    "                        round(res.loglikelihood,1),\n",
    "                        round(res.aic,1),\n",
    "                        round(res.bic,1)]\n",
    "        EGARCH_table.append(summary_table_row)\n",
    "        print(\"EGARCH\")\n",
    "        print(summary_short.iloc[0:])\n",
    "        #print(summary_table_row)\n",
    "        #print(res.summary())\n",
    "        print(\"\\n\")\n",
    "\n",
    "#log of variants\n",
    "#EGARCH 1,1,1 normal - 2* 6** 2***           \n",
    "#EGARCH p,o,q normal - didnt try\n",
    "#\n",
    "#EGARCH 1,1,1 studentst - 1/ 6* 3**           \n",
    "#EGARCH p,o,q studentst - didnt try\n",
    "#\n",
    "#EGARCH 1,1,1 skewt - 6* 2** 2***         \n",
    "#EGARCH p,o,q skewt - didnt try ###Overall conclusion - I will probablz go with skewt, although it was close with normal\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "#print(table)    \n",
    "#table.to_csv(r\"D:\\Dokumenty\\Vejška\\Magisterské studium\\DIPLOMKA\\Code_and_Data\\Data_scraping\\DEX_data_scraper\\complete_data\\returns_5min.csv\", index=False)\n",
    "if GARCH_switch:\n",
    "    GARCH_table = pd.DataFrame(GARCH_table)\n",
    "    GARCH_table.columns = ['Token', 'mu', 'omega', 'alpha', 'beta', 'alpha+beta', 'LogLikelihood', 'AIC', 'BIC']\n",
    "    print(GARCH_table)\n",
    "    #GARCH_table.to_csv(r\"D:\\Dokumenty\\Vejška\\Magisterské studium\\DIPLOMKA\\Tables\\GARCH_table.csv\", index=False)\n",
    "\n",
    "if GJRGARCH_switch:\n",
    "    GJR_GARCH_table = pd.DataFrame(GJR_GARCH_table)\n",
    "    #GJR_GARCH_table.columns = ['Token', 'mu', 'mu_p-val', 'omega', 'omega_p-val', 'alpha', 'alpha_p-val', 'gamma', 'gamma_p-val', 'beta', 'beta_p-val', 'R2', 'LogLikelihood', 'AIC', 'BIC']\n",
    "    GJR_GARCH_table.columns=['Token', 'mu', 'omega', 'alpha', 'gamma', 'beta', 'alpha+beta', 'LogLikelihood', 'AIC', 'BIC']\n",
    "    print(GJR_GARCH_table)\n",
    "    #GJR_GARCH_table.to_csv(r\"D:\\Dokumenty\\Vejška\\Magisterské studium\\DIPLOMKA\\Tables\\GJR_GARCH_table.csv\", index=False)\n",
    "\n",
    "if EGARCH_switch:\n",
    "    EGARCH_table = pd.DataFrame(EGARCH_table)\n",
    "    EGARCH_table.columns = ['Token', 'mu', 'omega', 'alpha', 'gamma', 'beta', 'alpha+beta', 'LogLikelihood', 'AIC', 'BIC']\n",
    "    print(EGARCH_table)\n",
    "    EGARCH_table.to_csv(r\"D:\\Dokumenty\\Vejška\\Magisterské studium\\DIPLOMKA\\Tables\\EGARCH_table.csv\", index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideas\n",
    "\n",
    "#### 4/10/23\n",
    "\n",
    "What I see as a problems right now:\n",
    "\n",
    "- I have unevenly spaced trade data.\n",
    "\n",
    "- Returns I see are sometimes as expected, sometimes there is the buy-sell (spread) gap.\n",
    "\n",
    "Solutions:\n",
    "\n",
    "- spacing: Transform data to unified time intervals. Either 5 minutes, or play along with the blockchain timing and set interval 12 sec on ETH and 3 on BSC -- verify with the histogram of time interval lenghts. The reason is that my focus is now just describe how returns behave, not some fancy durations stuf. This Would give me a solid base how the returns could be described and I could continue to the *simulation*.\n",
    "\n",
    "- Spread gap: Either transform data somehow, or leave as \"feature\", not a bug\", and attempt to simulate it in the Agent model\n",
    "\n",
    "ToDo next:\n",
    "\n",
    "- ~~Plot the histogram of observation spacings~~ -- done, they are mostly close to sub 1 minute (majority between 0 and 50 seconds). It seems that it might be reasonable to set the rescale time interval according to blocks, in ETH a lot of trades is spaced between 8 and 15 seconds, while in BSC it is sub 8 (rough estinates, just based on histogram observations). This aligns with the blocktimes.\n",
    "\n",
    "- ~~Try rescaling data with 12 and 3 second intervals~~ -- tried 12s and it seems like too much, at least for histograms. The plots of prices and returns in time look very similar, in histogram any other values than 0 have almost no bars. The 3 seconds very similar.\n",
    "\n",
    "- ~~get the Jarque berra test and ADF test for 5Min transformation, just to verify that it is sound~~ -- it is, super significantly both rejected\n",
    "\n",
    "- ~~Plot ACF and PACF ((partial)autocorrelation function) - read the kaggle post + define it as a function~~ -- done\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4/11/23\n",
    "\n",
    "What have I done:\n",
    "\n",
    "- I have done the TODOs from yesterday. \n",
    " \n",
    "  * I have played with the ACF and PACF, but later realized I might not need them. I am not sure yet.\n",
    "  * Rescaling for respective blocktimes -- this seems like too small interval. Probably will stick to 5Min. Although from histogram of observation spaces it seemed like it would be sound, as there are many observation spaced out roughly by the blocktime.\n",
    "- I further analyzed returns - Durbin-Watson test for autocorrelation of returns and absolute returns. It moreless fits the stylized facts, small deviation.\n",
    "\n",
    "What I see as a problems right now:\n",
    "\n",
    "- I would like to move to either Describing analyzis of returns in DP text or to simulation. For that I need to answer the two problems I see:\n",
    "  * Unevenly spaced data, is 5 minute aggregation good idea\n",
    "  * The spread gap on some pairs -- I still have no idea what to do with this\n",
    "\n",
    "Solutions:\n",
    "\n",
    "- Write email to LK asap. Tomorow? Perhaps no need for consultation if I describe what I think I should do and LK agrees? Before sending email remains to do the table as in the paper.\n",
    "\n",
    "ToDo next:\n",
    "- ~~Add method for calculating realized variance~~ -- only possible for aggregated data from definition? - clarify!\n",
    "  * Notes from research on this: RV is calculated as a sum of returns over some time period. Thus, the unevenly spaced observations could affect this, as more transactions would lead to cumulation of returns at certain points (*but isnt that interesting as well??*). \n",
    "  * ~~Try these three options:~~ -- done\n",
    "    * Do the five minute interval and sum the returns, no special treatment\n",
    "    * Sum returns over five minute intervals and divide the sum by number of observations summed\n",
    "    * Sum returns over longer periods (1 hour?), based on 5-min aggregated returns\n",
    "- ~~Add the DW test for Realized Variance~~\n",
    "- ~~Do the overview table~~\n",
    "- Clarify whether ACF and PACF will be useful (perhaps in analyzing the GARCH effects in returns??)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4/12/23\n",
    "\n",
    "What have I done:\n",
    "\n",
    "- I have done the Overview Tables following the example from the paper\n",
    "- I have done the Realized Variance in three ways\n",
    "\n",
    "ToDo tommorow:\n",
    "- ~~Add to the overview table n of obs~~\n",
    "- ~~Interpret the results of RV, run it through the DW test~~\n",
    "- ~~Check for other rests. Perhaps do the aggregated figure.~~\n",
    "- ~~Write email to LK.~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4/13/23\n",
    "\n",
    "What have I done:\n",
    "\n",
    "- Added Observations column to Overview table\n",
    "- Looked one more time into the spread gap. It seems like there is a hidden constant by which the price changes when the \"sentiment\" changes.\n",
    "- I have done plots and DW test for all three modes of Realized Variance\n",
    "    - From the plots, the mode 3 (Calculating RV based on aggregated prices and returns, as metaaggreagetion 5 min -> 1 hour), looks best, I will probably stick to this mode.\n",
    "    - Also the third mode has the best results on autocorrelation of Realized Volatility (sqrt of Realized Variance) -- all are autocorrelated, even HEX.\n",
    "    - The other methods mostly also ok, but mode 3 has the highest autocorrelation and the plots look as expected the most.\n",
    "- Went through the last meeting notes.\n",
    "- Wrote email to LK and asked for meeting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4/14/23\n",
    "\n",
    "What have I done:\n",
    "- Made the plot look nicer\n",
    "\n",
    "TODO:\n",
    "- ~~Plan what to do next~~\n",
    "- ~~Add standard deviation to the table For sure~~\n",
    "- ~~Perhaps also add the KPSS test?~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4/16/23\n",
    "\n",
    "What have I done:\n",
    "- TODOs from before\n",
    "- planned what to do next, found some papers to go through, defined what to look for in them - what things I could analyze in my data and then try to simulate.\n",
    "- Went through 3/6 papers.\n",
    "\n",
    "What to do next:\n",
    "- Continue with the papers.\n",
    "- Start with the section on downloading data in text.\n",
    "- Mybe add table with basic \"tokenomics\" at the end of token-description list in text(??).\n",
    "- Look at Ljung-Box test."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5/5/23\n",
    "\n",
    "What have I done:\n",
    "- Started with GARCH models\n",
    " - Got the basic specification of GARCH, GJR-GARCH, EGARCH\n",
    " - started looking for optimal specification\n",
    "    - In GARCH, it seems that 1,1 lags and normal or skewt dist will work best\n",
    "\n",
    "What to do next:\n",
    "- Repeat the searching procedure for EGARCH and GJR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5/6/23\n",
    "\n",
    "What have I done:\n",
    "- Tried different assumed distributions for GARCH models\n",
    "    - Skewt probably best, however normal closely follows\n",
    "    - looked at two papers who report GARCH - Zhang and Martin. Martin has nice table for GARCHes\n",
    "- Played with the Ljung Box test again, added the test for lags from 1 to 15\n",
    "    - New Idea - I think it should probably be mentioned as well, as it is widely used, so I can mention it in just words and say\n",
    "        - on x tokens rejected for both returns and squred returns\n",
    "        - on tokens abc rejected only on squared\n",
    "        - on tokens def rejected only on returns (unexpected)\n",
    "        - So, for additional insight, the ACF plots can be displayed\n",
    "\n",
    "\n",
    "\n",
    "What to do next:\n",
    "- Continue with text. Implement the changes with Ljung Box test to methodology, describe GARCH models, start reporting results in chapter 5."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
